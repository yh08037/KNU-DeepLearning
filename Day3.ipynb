{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day3",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yh08037/KNU-DeepLearning/blob/master/Day3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "gqV4bTXdTx2t",
        "colab_type": "code",
        "outputId": "f79f1df2-0cdd-49aa-d71b-10cacfcf2d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrvie')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrvie; to attempt to forcibly remount, call drive.mount(\"/content/gdrvie\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ergoCcHUcIjb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MLP Iris with Dataload"
      ]
    },
    {
      "metadata": {
        "id": "pbsN5bnZUCWf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math as m\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.nn as nn\n",
        "\n",
        "# data loading from csv file\n",
        "data = np.loadtxt('/content/gdrive/My Drive/Colab Notebooks/Iris.csv', delimiter=',', unpack=True, dtype='float32')\n",
        "data = np.transpose(data)\n",
        "x_vals = np..array([x[0:3] for x in data])\n",
        "y_vals = np..array([x[3] for x in data])\n",
        "\n",
        "# 80% for training, 20% for test\n",
        "train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\n",
        "test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\n",
        "x_vals_train = x_vals[train_indices]\n",
        "x_vals_test = x_vals[test_indices]\n",
        "y_vals_train = y_vals[train_indices]\n",
        "y_vals_test = y_vals[test_indices]\n",
        "\n",
        "# Input Data Normalization (from -1 to 1)\n",
        "def normalize_cols(m):\n",
        "    col_max = m.max(axis=0)\n",
        "    col_min = m.min(axis=0)\n",
        "    return (m-col_min)/(col_max - col_min)\n",
        "\n",
        "x_vals_train = np.nan_to_num(normalize_cols(x_vals_train))\n",
        "x_vals_test = np.nan_to_num(normalize_cols(x_vals_test))\n",
        "\n",
        "# Model set-up\n",
        "x_data = tf.placeholder(shape=[None,3], dtype = tf.float32)\n",
        "y_target = tf.placeholder(shape=[None,1], dtype = tf.float32)\n",
        "\n",
        "H_nodes = 10\n",
        "\n",
        "W1 = tf.Variable(tf.random_normal(shape=[3,H_nodes]))\n",
        "b1 = tf.Variable(tf.random_normal(shape=[H_nodes]))\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal(shape=[H_nodes,1]))\n",
        "b2 = tf.Variable(tf.random_normal(shape=[1]))\n",
        "\n",
        "H_out = tf.nn.relu(tf.add(tf.matmul(x_data,W1),b1))\n",
        "L_out = tf.nn.relu(tf.add(tf.matmul(H_out,W2),b2))\n",
        "\n",
        "# loss and optimizer definition\n",
        "#loss = tf.reduce_mean(tf.square(y_target - L_out))\n",
        "loss = tf.nn.l2_loss(y_target - L_out)\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(0.0001)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "# Running session\n",
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "# Create a saver object which will save all the variables\n",
        "saver = tf.train.Saver()\n",
        "loss_vec = []\n",
        "test_loss = []\n",
        "\n",
        "for i in range(2000):\n",
        "    sess.run(train_op, feed_dict={x_data:x_vals_train, y_target:np.transpose([y_vals_train])})\n",
        "\n",
        "    temp_loss = sess.run(loss, feed_dict={x_data:x_vals_train, y_target:np.transpose([y_vals_train])})\n",
        "    loss_vec.append(np.sqrt(temp_loss))\n",
        "    \n",
        "    test_temp_loss = sess.run(loss, feed_dict={x_data:x_vals_test, y_target:np.transpose([y_vals_test])})\n",
        "    test_loss.append(np.sqrt(test_temp_loss))\n",
        "    if (i+1)%100==0:\n",
        "        print(i+1, temp_loss, test_temp_loss)\n",
        "# Now, save the graph\n",
        "saver.save(sess, '/content/gdrive/My Drive/Colab Notebooks/model/irisMLP', global_step=1000)\n",
        "\n",
        "plt.plot(loss_vec, label='train loss')\n",
        "plt.plot(test_loss, label='test loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jBHVj1SvbaRd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(입력값 3개 / bias1) - W1 -(히든노드 10개 / bias2) - W2 - H -L\n",
        "\n",
        "# MNIST CNN\n",
        "\n",
        "Optical Charcter Recognition Problem\n"
      ]
    },
    {
      "metadata": {
        "id": "XJ9urRvQaNx3",
        "colab_type": "code",
        "outputId": "fd0fa289-2764-4841-b5e7-b9e89928132e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the necessary package\n",
        "## Packages and Data\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets.mnist import load_data\n",
        "\n",
        "## Load MNIST data\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_data()\n",
        "\n",
        "## Transform to 4-dim tensors\n",
        "\n",
        "# print(x_train.shape)    # (60000, 28, 28)\n",
        "x_train = np.expand_dims(x_train, axis=-1)   # axis=-1 : 텐서의 맨 뒤(인덱스:-1)를 expand\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "# print(x_train.shape)    # (60000, 28, 28, 1)\n",
        "\n",
        "## Normalization [0, 255] -> [0,1]\n",
        "print(x_train.max())\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "# #Allocate placeholders for I/O\n",
        "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int64, shape=[None])\n",
        "\n",
        "## Integer label -> Binary, one-hot label\n",
        "y_onehot = tf.one_hot(y, 10)\n",
        "print(y_onehot.shape)\n",
        "\n",
        "## Dropout probability\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# Network Setting\n",
        "## Convolution layer 1\n",
        "\n",
        "# tf.truncated_normal : 2시그마 내의 정규분포\n",
        "W_conv1 = tf.Variable(tf.truncated_normal(shape=[5, 5, 1, 64], stddev=5e-2)) \n",
        "# [5, 5, 1, 64] : 흑백이라서 1 => 컬러면 rgb 채널 3개 추가 : [5, 5, 4, 64]\n",
        "b_conv1 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
        "h_conv1 = tf.nn.relu(tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
        "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "\n",
        "## Convolution layer 2\n",
        "W_conv2 = tf.Variable(tf.truncated_normal(shape=[5, 5, 64, 64], stddev=5e-2))\n",
        "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
        "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
        "h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "## Fully connedted layer 1\n",
        "W_fc1 = tf.Variable(tf.truncated_normal(shape=[7*7*64, 384], stddev=5e-2))\n",
        "b_fc1 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob=keep_prob)\n",
        "\n",
        "## Fully connected layer 2\n",
        "W_fc2 = tf.Variable(tf.truncated_normal(shape=[384, 10], stddev=5e-2))\n",
        "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
        "logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
        "y_pred = tf.nn.softmax(logits)\n",
        "\n",
        "# LOSS AND OPTIMIZER\n",
        "# Cross entropy loss with doftmax output\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_onehot, logits=logits))\n",
        "\n",
        "# Adam optimizer\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
        "\n",
        "#Accuracy calculation\n",
        "correct_prediction = tf.equal(tf.argmax(y_pred, 1), y)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# LEARGNING STEPS\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(10000):\n",
        "        idx = np.random.randint(x_train.shape[0], size=128)\n",
        "        batch = (x_train[idx], y_train[idx])\n",
        "        if i%100==0:\n",
        "            train_accuracy = sess.run(accuracy, feed_dict={x:batch[0], y:batch[1], keep_prob:1.})\n",
        "            loss_print = loss.eval(feed_dict={x:batch[0], y:batch[1], keep_prob:1.})\n",
        "            print(\"step:%d, acc:%f, loss:%f\" %(i, train_accuracy, loss_print))\n",
        "        sess.run(train_step, feed_dict={x:batch[0], y:batch[1], keep_prob:0.5})\n",
        "    test_accuracy = accuracy.eval({x:x_test, y:y_test, keep_prob:1.})\n",
        "    print(\"test acc:%f\" %test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "255\n",
            "(?, 10)\n",
            "step:0, acc:0.093750, loss:2.485717\n",
            "step:100, acc:0.820312, loss:0.744914\n",
            "step:200, acc:0.929688, loss:0.275824\n",
            "step:300, acc:0.921875, loss:0.310555\n",
            "step:400, acc:0.953125, loss:0.225718\n",
            "step:500, acc:0.968750, loss:0.170144\n",
            "step:600, acc:0.945312, loss:0.162272\n",
            "step:700, acc:0.953125, loss:0.154553\n",
            "step:800, acc:0.968750, loss:0.099658\n",
            "step:900, acc:0.945312, loss:0.157109\n",
            "step:1000, acc:0.960938, loss:0.127328\n",
            "step:1100, acc:0.976562, loss:0.080801\n",
            "step:1200, acc:0.976562, loss:0.126921\n",
            "step:1300, acc:0.960938, loss:0.132873\n",
            "step:1400, acc:0.968750, loss:0.089056\n",
            "step:1500, acc:0.960938, loss:0.112677\n",
            "step:1600, acc:0.976562, loss:0.072304\n",
            "step:1700, acc:0.945312, loss:0.144309\n",
            "step:1800, acc:0.984375, loss:0.071153\n",
            "step:1900, acc:0.984375, loss:0.034536\n",
            "step:2000, acc:0.992188, loss:0.022598\n",
            "step:2100, acc:0.976562, loss:0.071021\n",
            "step:2200, acc:0.976562, loss:0.064937\n",
            "step:2300, acc:0.968750, loss:0.075585\n",
            "step:2400, acc:0.992188, loss:0.019639\n",
            "step:2500, acc:0.984375, loss:0.067058\n",
            "step:2600, acc:0.992188, loss:0.041454\n",
            "step:2700, acc:0.976562, loss:0.068543\n",
            "step:2800, acc:0.984375, loss:0.061713\n",
            "step:2900, acc:0.984375, loss:0.034250\n",
            "step:3000, acc:0.992188, loss:0.023666\n",
            "step:3100, acc:0.992188, loss:0.023856\n",
            "step:3200, acc:0.992188, loss:0.039401\n",
            "step:3300, acc:1.000000, loss:0.013541\n",
            "step:3400, acc:0.968750, loss:0.081111\n",
            "step:3500, acc:0.976562, loss:0.064586\n",
            "step:3600, acc:0.992188, loss:0.020660\n",
            "step:3700, acc:0.984375, loss:0.027624\n",
            "step:3800, acc:1.000000, loss:0.009114\n",
            "step:3900, acc:0.984375, loss:0.035953\n",
            "step:4000, acc:0.976562, loss:0.065646\n",
            "step:4100, acc:1.000000, loss:0.011913\n",
            "step:4200, acc:0.984375, loss:0.038163\n",
            "step:4300, acc:0.984375, loss:0.044625\n",
            "step:4400, acc:1.000000, loss:0.003926\n",
            "step:4500, acc:0.992188, loss:0.029751\n",
            "step:4600, acc:0.992188, loss:0.016061\n",
            "step:4700, acc:0.992188, loss:0.015355\n",
            "step:4800, acc:1.000000, loss:0.005870\n",
            "step:4900, acc:1.000000, loss:0.008060\n",
            "step:5000, acc:0.976562, loss:0.037298\n",
            "step:5100, acc:0.992188, loss:0.023527\n",
            "step:5200, acc:1.000000, loss:0.007751\n",
            "step:5300, acc:1.000000, loss:0.006556\n",
            "step:5400, acc:1.000000, loss:0.013585\n",
            "step:5500, acc:1.000000, loss:0.007966\n",
            "step:5600, acc:0.992188, loss:0.034659\n",
            "step:5700, acc:1.000000, loss:0.004290\n",
            "step:5800, acc:0.984375, loss:0.055059\n",
            "step:5900, acc:0.992188, loss:0.021166\n",
            "step:6000, acc:1.000000, loss:0.010790\n",
            "step:6100, acc:0.992188, loss:0.023475\n",
            "step:6200, acc:1.000000, loss:0.009275\n",
            "step:6300, acc:0.992188, loss:0.028567\n",
            "step:6400, acc:1.000000, loss:0.004991\n",
            "step:6500, acc:1.000000, loss:0.008224\n",
            "step:6600, acc:0.992188, loss:0.027380\n",
            "step:6700, acc:0.992188, loss:0.014973\n",
            "step:6800, acc:0.976562, loss:0.052084\n",
            "step:6900, acc:0.992188, loss:0.015024\n",
            "step:7000, acc:1.000000, loss:0.004285\n",
            "step:7100, acc:0.992188, loss:0.012318\n",
            "step:7200, acc:1.000000, loss:0.008484\n",
            "step:7300, acc:1.000000, loss:0.000784\n",
            "step:7400, acc:1.000000, loss:0.001492\n",
            "step:7500, acc:0.992188, loss:0.015466\n",
            "step:7600, acc:1.000000, loss:0.006357\n",
            "step:7700, acc:0.992188, loss:0.015178\n",
            "step:7800, acc:0.992188, loss:0.018948\n",
            "step:7900, acc:1.000000, loss:0.001225\n",
            "step:8000, acc:1.000000, loss:0.004128\n",
            "step:8100, acc:0.992188, loss:0.016498\n",
            "step:8200, acc:1.000000, loss:0.006977\n",
            "step:8300, acc:0.992188, loss:0.025281\n",
            "step:8400, acc:1.000000, loss:0.004721\n",
            "step:8500, acc:0.976562, loss:0.070166\n",
            "step:8600, acc:1.000000, loss:0.006751\n",
            "step:8700, acc:1.000000, loss:0.005852\n",
            "step:8800, acc:1.000000, loss:0.002029\n",
            "step:8900, acc:0.984375, loss:0.043715\n",
            "step:9000, acc:1.000000, loss:0.004679\n",
            "step:9100, acc:1.000000, loss:0.005521\n",
            "step:9200, acc:1.000000, loss:0.003358\n",
            "step:9300, acc:0.992188, loss:0.008781\n",
            "step:9400, acc:0.992188, loss:0.013248\n",
            "step:9500, acc:0.984375, loss:0.033759\n",
            "step:9600, acc:0.992188, loss:0.013423\n",
            "step:9700, acc:0.992188, loss:0.012031\n",
            "step:9800, acc:1.000000, loss:0.005143\n",
            "step:9900, acc:1.000000, loss:0.003354\n",
            "test acc:0.992400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BdaoYKs1bB5F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# fashion MNIST CNN"
      ]
    },
    {
      "metadata": {
        "id": "IDHM5Tm6bLl8",
        "colab_type": "code",
        "outputId": "b7023fa7-16a1-4dec-d7e4-aeacdede9d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2017
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
        "\n",
        "\n",
        "\n",
        "# Packages and Data\n",
        "\n",
        "# Load fashion MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = load_data()\n",
        "print(x_train.shape)   # (60000, 28, 28) 3개 => 4개로 확장하기\n",
        "\n",
        "# Transform to 4-dim tensors\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "# Normalization : [0, 255] => [0, 1]\n",
        "print(x_train.max())   # 255\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "# Allocate placeholders for I/O\n",
        "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int64, shape=[None])\n",
        "\n",
        "# Integer label => Binary, one-hot label\n",
        "y_onehot = tf.one_hot(y, 10)\n",
        "print(y_onehot.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Network Setting\n",
        "\n",
        "# Dropout porbability\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# Convolution layer 1\n",
        "W_conv1 = tf.Variable(tf.truncated_normal(shape=[5, 5, 1, 64], stddev=5e-2))\n",
        "b_conv1 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
        "h_conv1 = tf.nn.relu(tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
        "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "#Convolution layer 2\n",
        "W_conv2 = tf.Variable(tf.truncated_normal(shape=[5, 5, 64, 64], stddev=5e-2))\n",
        "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
        "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
        "h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "# Fully connected layer 1\n",
        "W_fc1 = tf.Variable(tf.truncated_normal(shape=[7 * 7 * 64, 384], stddev=5e-2))\n",
        "b_fc1 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob=keep_prob)\n",
        "\n",
        "# Fully connected layer 2\n",
        "W_fc2 = tf.Variable(tf.truncated_normal(shape=[384, 10], stddev=5e-2))\n",
        "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
        "logits = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
        "y_pred = tf.nn.softmax(logits)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Loss and Optimizer\n",
        "\n",
        "# Cross entropy loss with softmax output\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_onehot, logits=logits))\n",
        "\n",
        "# Adam optimizer\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
        "\n",
        "# Accuracy calculation\n",
        "correct_prediction = tf.equal(tf.argmax(y_pred, 1), y)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Learning Steps\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(10000):\n",
        "        idx = np.random.randint(x_train.shape[0], size=128)\n",
        "        batch = (x_train[idx], y_train[idx])\n",
        "        if i % 100 == 0:\n",
        "            train_accuracy = sess.run(accuracy, feed_dict={x: batch[0], y: batch[1], keep_prob: 1.})\n",
        "            loss_print = loss.eval(feed_dict={x: batch[0], y: batch[1], keep_prob: 1.})\n",
        "            print(\"step: %d, acc: %f, loss: %f\" % (i, train_accuracy, loss_print))\n",
        "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
        "    test_accuracy = accuracy.eval(feed_dict={x: x_test, y: y_test, keep_prob: 1.})\n",
        "    print(\"test acc: %f\" % test_accuracy)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "255\n",
            "(?, 10)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-6-050ce8b9a060>:55: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "step: 0, acc: 0.093750, loss: 2.475114\n",
            "step: 100, acc: 0.648438, loss: 0.986481\n",
            "step: 200, acc: 0.765625, loss: 0.668589\n",
            "step: 300, acc: 0.796875, loss: 0.593687\n",
            "step: 400, acc: 0.734375, loss: 0.782823\n",
            "step: 500, acc: 0.742188, loss: 0.582953\n",
            "step: 600, acc: 0.828125, loss: 0.541704\n",
            "step: 700, acc: 0.757812, loss: 0.615698\n",
            "step: 800, acc: 0.875000, loss: 0.333548\n",
            "step: 900, acc: 0.843750, loss: 0.413103\n",
            "step: 1000, acc: 0.851562, loss: 0.410763\n",
            "step: 1100, acc: 0.875000, loss: 0.360144\n",
            "step: 1200, acc: 0.843750, loss: 0.511507\n",
            "step: 1300, acc: 0.906250, loss: 0.274410\n",
            "step: 1400, acc: 0.867188, loss: 0.404584\n",
            "step: 1500, acc: 0.890625, loss: 0.344413\n",
            "step: 1600, acc: 0.835938, loss: 0.458911\n",
            "step: 1700, acc: 0.820312, loss: 0.476701\n",
            "step: 1800, acc: 0.882812, loss: 0.302617\n",
            "step: 1900, acc: 0.914062, loss: 0.307416\n",
            "step: 2000, acc: 0.882812, loss: 0.386772\n",
            "step: 2100, acc: 0.890625, loss: 0.309396\n",
            "step: 2200, acc: 0.843750, loss: 0.487345\n",
            "step: 2300, acc: 0.882812, loss: 0.384104\n",
            "step: 2400, acc: 0.890625, loss: 0.326500\n",
            "step: 2500, acc: 0.859375, loss: 0.347945\n",
            "step: 2600, acc: 0.906250, loss: 0.329150\n",
            "step: 2700, acc: 0.921875, loss: 0.231501\n",
            "step: 2800, acc: 0.890625, loss: 0.292877\n",
            "step: 2900, acc: 0.914062, loss: 0.252262\n",
            "step: 3000, acc: 0.851562, loss: 0.374776\n",
            "step: 3100, acc: 0.914062, loss: 0.236418\n",
            "step: 3200, acc: 0.906250, loss: 0.245795\n",
            "step: 3300, acc: 0.851562, loss: 0.332253\n",
            "step: 3400, acc: 0.867188, loss: 0.366444\n",
            "step: 3500, acc: 0.890625, loss: 0.406144\n",
            "step: 3600, acc: 0.890625, loss: 0.294306\n",
            "step: 3700, acc: 0.898438, loss: 0.267011\n",
            "step: 3800, acc: 0.898438, loss: 0.313162\n",
            "step: 3900, acc: 0.843750, loss: 0.359953\n",
            "step: 4000, acc: 0.906250, loss: 0.284279\n",
            "step: 4100, acc: 0.914062, loss: 0.276397\n",
            "step: 4200, acc: 0.914062, loss: 0.256201\n",
            "step: 4300, acc: 0.953125, loss: 0.165533\n",
            "step: 4400, acc: 0.945312, loss: 0.189367\n",
            "step: 4500, acc: 0.921875, loss: 0.256933\n",
            "step: 4600, acc: 0.851562, loss: 0.370667\n",
            "step: 4700, acc: 0.937500, loss: 0.178990\n",
            "step: 4800, acc: 0.968750, loss: 0.145359\n",
            "step: 4900, acc: 0.882812, loss: 0.267971\n",
            "step: 5000, acc: 0.898438, loss: 0.331826\n",
            "step: 5100, acc: 0.906250, loss: 0.280695\n",
            "step: 5200, acc: 0.937500, loss: 0.180835\n",
            "step: 5300, acc: 0.921875, loss: 0.235426\n",
            "step: 5400, acc: 0.937500, loss: 0.234101\n",
            "step: 5500, acc: 0.937500, loss: 0.199997\n",
            "step: 5600, acc: 0.914062, loss: 0.243710\n",
            "step: 5700, acc: 0.937500, loss: 0.206027\n",
            "step: 5800, acc: 0.890625, loss: 0.230130\n",
            "step: 5900, acc: 0.906250, loss: 0.334191\n",
            "step: 6000, acc: 0.929688, loss: 0.212005\n",
            "step: 6100, acc: 0.882812, loss: 0.301185\n",
            "step: 6200, acc: 0.898438, loss: 0.215077\n",
            "step: 6300, acc: 0.898438, loss: 0.337136\n",
            "step: 6400, acc: 0.898438, loss: 0.246192\n",
            "step: 6500, acc: 0.914062, loss: 0.240680\n",
            "step: 6600, acc: 0.867188, loss: 0.300071\n",
            "step: 6700, acc: 0.906250, loss: 0.237773\n",
            "step: 6800, acc: 0.921875, loss: 0.183455\n",
            "step: 6900, acc: 0.921875, loss: 0.237061\n",
            "step: 7000, acc: 0.921875, loss: 0.208006\n",
            "step: 7100, acc: 0.882812, loss: 0.275171\n",
            "step: 7200, acc: 0.921875, loss: 0.233677\n",
            "step: 7300, acc: 0.906250, loss: 0.225095\n",
            "step: 7400, acc: 0.890625, loss: 0.273714\n",
            "step: 7500, acc: 0.921875, loss: 0.202732\n",
            "step: 7600, acc: 0.906250, loss: 0.257618\n",
            "step: 7700, acc: 0.898438, loss: 0.263308\n",
            "step: 7800, acc: 0.945312, loss: 0.233462\n",
            "step: 7900, acc: 0.937500, loss: 0.185021\n",
            "step: 8000, acc: 0.929688, loss: 0.230896\n",
            "step: 8100, acc: 0.914062, loss: 0.249604\n",
            "step: 8200, acc: 0.914062, loss: 0.216070\n",
            "step: 8300, acc: 0.882812, loss: 0.239678\n",
            "step: 8400, acc: 0.890625, loss: 0.223302\n",
            "step: 8500, acc: 0.906250, loss: 0.253878\n",
            "step: 8600, acc: 0.960938, loss: 0.115411\n",
            "step: 8700, acc: 0.929688, loss: 0.225636\n",
            "step: 8800, acc: 0.882812, loss: 0.305894\n",
            "step: 8900, acc: 0.906250, loss: 0.259061\n",
            "step: 9000, acc: 0.921875, loss: 0.219601\n",
            "step: 9100, acc: 0.937500, loss: 0.233860\n",
            "step: 9200, acc: 0.906250, loss: 0.230181\n",
            "step: 9300, acc: 0.929688, loss: 0.237906\n",
            "step: 9400, acc: 0.953125, loss: 0.163189\n",
            "step: 9500, acc: 0.945312, loss: 0.240240\n",
            "step: 9600, acc: 0.929688, loss: 0.257779\n",
            "step: 9700, acc: 0.937500, loss: 0.193066\n",
            "step: 9800, acc: 0.898438, loss: 0.230458\n",
            "step: 9900, acc: 0.921875, loss: 0.206537\n",
            "test acc: 0.906700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FrikVapZbpkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vhk4Qw0IkdK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ksize : 커널 사이즈, pooling 할 때 필터같은 틀의 크기. 중간의 두 숫자가 가로세로\n",
        "\n",
        "strides : convolution, pooling 할 때 필터(틀)이 움직이는 칸 수. 중간의 두 숫자가 가로세로\n",
        "\n"
      ]
    }
  ]
}